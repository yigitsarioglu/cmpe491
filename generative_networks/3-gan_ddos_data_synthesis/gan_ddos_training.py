# -*- coding: utf-8 -*-
"""gan_ddos_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_PfLSrdeV5wfpbxAlJhGXPB8S1GRDIK7
"""

!pip install -q pandas scikit-learn matplotlib

import os, time, pathlib
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn

import shutil
from google.colab import drive
drive.mount("/content/gdrive")
# Change the code below if the path to the dataset is different for you.
shutil.unpack_archive("/content/gdrive/MyDrive/ddos_dataset.zip", "/tmp/")

# Sadece Features (10 feature)
csv_path = "/tmp/dataset_sdn.csv"   # kendi yolunu yaz
df = pd.read_csv(csv_path, low_memory=False)
print(df.shape)
display(df.head())

SELECTED_FEATURES = [
    'dt','pktcount','bytecount','dur','tot_dur',
    'packetins','pktperflow','byteperflow','pktrate','tx_bytes'
]

df = df[SELECTED_FEATURES].dropna().reset_index(drop=True)
print("Using:", df.columns.tolist())

# Scale 0–1
scaler = MinMaxScaler()
X = scaler.fit_transform(df.values)
FEATURE_DIM = X.shape[1]
print("Feature dim:", FEATURE_DIM)

# Train/val split
X_train, X_val = train_test_split(X, test_size=0.1, random_state=42)

# Generator & Discriminator (Vanilla GAN)


import torch
import torch.nn as nn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

Z_DIM = 64
HIDDEN = 128
LR = 2e-4
BETAS = (0.5, 0.999)
BATCH_SIZE = 256
EPOCHS = 200

class Generator(nn.Module):
    def __init__(self, z_dim, out_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(z_dim, hidden),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden, hidden),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden, out_dim),
            nn.Sigmoid()
        )
    def forward(self, z):
        return self.net(z)

class Discriminator(nn.Module):
    def __init__(self, in_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden, hidden),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden, 1),
            nn.Sigmoid()
        )
    def forward(self, x):
        return self.net(x)

G = Generator(Z_DIM, FEATURE_DIM, HIDDEN).to(device)
D = Discriminator(FEATURE_DIM, HIDDEN).to(device)

optG = torch.optim.Adam(G.parameters(), lr=LR, betas=BETAS)
optD = torch.optim.Adam(D.parameters(), lr=LR, betas=BETAS)

criterion = nn.BCELoss()

train_loader = torch.utils.data.DataLoader(
    torch.tensor(X_train, dtype=torch.float32),
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True
)

history_D = []
history_G = []

for epoch in range(1, EPOCHS + 1):

    D_loss_epoch = 0.0
    G_loss_epoch = 0.0
    batch_count = 0

    for real in train_loader:
        real = real.to(device)
        B = real.size(0)
        batch_count += 1

        # ---------------------- Train Discriminator ----------------------
        D.zero_grad()

        real_target = torch.full((B, 1), 0.9, device=device)  # label smoothing
        fake_target = torch.zeros((B, 1), device=device)

        out_real = D(real)
        loss_real = criterion(out_real, real_target)

        z = torch.randn(B, Z_DIM, device=device)
        fake = G(z).detach()
        out_fake = D(fake)
        loss_fake = criterion(out_fake, fake_target)

        loss_D = (loss_real + loss_fake) / 2
        loss_D.backward()
        optD.step()

        # ---------------------- Train Generator ----------------------
        G.zero_grad()
        z2 = torch.randn(B, Z_DIM, device=device)
        gen = G(z2)
        pred = D(gen)

        loss_G = criterion(pred, real_target)
        loss_G.backward()
        optG.step()

        # accumulate losses
        D_loss_epoch += loss_D.item()
        G_loss_epoch += loss_G.item()

    # ----------- Epoch average loss -----------
    avg_D = D_loss_epoch / batch_count
    avg_G = G_loss_epoch / batch_count

    history_D.append(avg_D)
    history_G.append(avg_G)

    if epoch % 5 == 0:
        print(f"Epoch {epoch}/{EPOCHS}  D={avg_D:.4f}  G={avg_G:.4f}")

plt.figure(figsize=(10, 5))
plt.plot(history_D, label="Discriminator Loss", linewidth=2)
plt.plot(history_G, label="Generator Loss", linewidth=2)
plt.title("GAN Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Synthetic Data Production


def generate_synthetic(n=5000):
    G.eval()
    with torch.no_grad():
        z = torch.randn(n, Z_DIM, device=device)
        syn = G(z).cpu().numpy()
        syn_inv = scaler.inverse_transform(syn)
        return syn_inv

synthetic = generate_synthetic(5000)
synthetic_df = pd.DataFrame(synthetic, columns=SELECTED_FEATURES)
print("Synthetic shape:", synthetic_df.shape)

synthetic_df.to_csv("synthetic_gan.csv", index=False)
print("Saved synthetic_gan.csv")

# Feature Similarity Metrics

# (KS-stat, KS-pvalue, KL divergence, Wasserstein)

from scipy.stats import ks_2samp, entropy, wasserstein_distance

real_norm = pd.DataFrame(scaler.transform(df), columns=df.columns)
syn_norm = pd.DataFrame(scaler.transform(synthetic_df), columns=df.columns)

metrics = []
for col in SELECTED_FEATURES:
    real_col = real_norm[col]
    syn_col  = syn_norm[col]

    ks_stat, ks_p = ks_2samp(real_col, syn_col)

    hist_real = np.histogram(real_col, bins=50, density=True)[0] + 1e-6
    hist_syn  = np.histogram(syn_col,  bins=50, density=True)[0] + 1e-6
    kl = entropy(hist_real, hist_syn)

    w = wasserstein_distance(real_col, syn_col)

    metrics.append({
        "Feature": col,
        "KS_stat": round(ks_stat,4),
        "KS_p": round(ks_p,4),
        "KL_divergence": round(kl,4),
        "Wasserstein": round(w,4)
    })

metrics_df = pd.DataFrame(metrics)
print(metrics_df.sort_values("KS_stat"))

# Machine Learning Utility

# TSTR → Train Synthetic, Test Real
# TRTS → Train Real, Test Synthetic


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# Add fake labels via clustering (unsupervised)
from sklearn.cluster import KMeans

kmeans_real = KMeans(n_clusters=2, random_state=42).fit(real_norm)
kmeans_syn = KMeans(n_clusters=2, random_state=42).fit(syn_norm)

real_y = kmeans_real.labels_
syn_y  = kmeans_syn.labels_

X_real = real_norm.values
X_syn  = syn_norm.values

# --------- TSTR ----------
clf = RandomForestClassifier(n_estimators=150, random_state=42)
clf.fit(X_syn, syn_y)
pred_real = clf.predict(X_real)

print("\n=== TSTR (Train Synthetic → Test Real) ===")
print("Accuracy:", accuracy_score(real_y, pred_real))
print("F1:", f1_score(real_y, pred_real))

# --------- TRTS ----------
clf2 = RandomForestClassifier(n_estimators=150, random_state=42)
clf2.fit(X_real, real_y)
pred_syn = clf2.predict(X_syn)

print("\n=== TRTS (Train Real → Test Synthetic) ===")
print("Accuracy:", accuracy_score(syn_y, pred_syn))
print("F1:", f1_score(syn_y, pred_syn))