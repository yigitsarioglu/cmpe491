# -*- coding: utf-8 -*-
"""kaggle_ddos_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KhElO-BmP5C-4qN1YxCFmXUHWApKaBWH
"""

#kaggle dataset ile feature selection /randomforest algoritması ile
# kaggle dataset :https://www.kaggle.com/datasets/aikenkazin/ddos-sdn-dataset/data

pip install --upgrade scikit-learn

# DATASET DOSYASINI GOOGLE DRİVE /tmp klasörüne unzip edilmesi
import shutil
from google.colab import drive

drive.mount("/content/gdrive")
# Change the code below if the path to the dataset is different for you.
shutil.unpack_archive("/content/gdrive/MyDrive/kaggle.zip", "/tmp/")

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns

# Filepath kontrol edilmesi
import os

file_path = r"/tmp/dataset_sdn.csv"
if os.path.exists(file_path):
    print("Dosya bulundu.")
else:
    print("Dosya bulunamadı.")

# CSV dosyası read edilir ayrıca kolon isimleri ve ilk satırları basılır
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# CSV dosyasını yükleme

data = pd.read_csv(r"/tmp/dataset_sdn.csv")

print("Kolon isimleri:")
print(data.columns)

# İlk birkaç satırı yazdırma (isteğe bağlı)
print("\nİlk birkaç satır:")
print(data.head())

print ( "Data shape" ,data.shape )
print ( data.info )

df = pd.read_csv(r"/tmp/dataset_sdn.csv", low_memory=False)
print("Orijinal satır x sütun:", df.shape)
display(df.head())

# 5) Hedef (target) kolonunu tahmin etme: sık kullanılan isimlere bak
possible_targets = ['label','Label','class','Class','attack','Attack','target','Target','flow_label']
target_col = None
for t in possible_targets:
    if t in df.columns:
        target_col = t
        break

if target_col is None:
    # Eğer bulamazsa, son kolonu hedef kabul et
    target_col = df.columns[-1]
    print(f"Standart hedef isimleri bulunamadı. Varsayılan olarak son sütunu hedef kabul ediyorum: {target_col}")
else:
    print("Tahmin edilen hedef kolonu:", target_col)

# 6) Basit veri temizleme + kategorik encoding
X = df.drop(columns=[target_col])
y = df[target_col].copy()

# Eğer hedef kategorik string ise, label encode et
if y.dtype == object:
    y = pd.factorize(y)[0]

# Kategorik sütunları one-hot yap (çok fazla kategori varsa dikkat)
obj_cols = X.select_dtypes(include=['object','category']).columns.tolist()
print("Kategorik sütunlar:", obj_cols)
if len(obj_cols) > 0:
    X = pd.get_dummies(X, columns=obj_cols, drop_first=True)

# Eksik değerleri medyan ile doldur
X = X.fillna(X.median(numeric_only=True))

# Eğer hala obj kol varsa (örnek: mixed types), numeric'e zorla
non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()
if len(non_numeric) > 0:
    print("Sayısal olmayan sütunlar tespit edildi, kaldırılıyor:", non_numeric)
    X = X.drop(columns=non_numeric)

print("Ön işleme sonrası X shape:", X.shape)

# 7) (Opsiyonel) Çok büyük dataset'lerde örnekleme

MAX_ROWS = 200000  # eğer dosya çok büyükse hız için küçült
if X.shape[0] > MAX_ROWS:
    print(f"Veri çok büyük ({X.shape[0]} satır). Rastgele {MAX_ROWS} satır örnekleniyor (stratify yoksa dengeden sapabilir).")
    sample_idx = np.random.RandomState(42).choice(X.index, size=MAX_ROWS, replace=False)
    X = X.loc[sample_idx].reset_index(drop=True)
    y = pd.Series(y).loc[sample_idx].reset_index(drop=True)
    print("Örnekleme sonrası shape:", X.shape)

# 8) Eğitim / test split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y))>1 and X.shape[0]>1 else None
)
print("Train / Test:", X_train.shape, X_test.shape)

# 9) Pipeline: StandardScaler + RandomForest
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('rf', RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42, class_weight='balanced'))
])

print("Model eğitiliyor (bu adım zaman alabilir)...")
pipe.fit(X_train, y_train)

# 10) Feature importances
rf = pipe.named_steps['rf']
importances = rf.feature_importances_
feat_names = X.columns
fi_df = pd.DataFrame({'feature': feat_names, 'importance': importances})
fi_df = fi_df.sort_values('importance', ascending=False).reset_index(drop=True)
display(fi_df.head(30))

# 11) Görselleştirme (top 30)
import matplotlib.pyplot as plt
top_n = 30
plt.figure(figsize=(10, max(4, 0.25*top_n)))
plt.barh(fi_df['feature'].head(top_n)[::-1], fi_df['importance'].head(top_n)[::-1])
plt.xlabel('Importance')
plt.title(f'Top {top_n} Feature Importances (Random Forest)')
plt.tight_layout()
plt.show()

# 12) SelectFromModel ile otomatik seçim (threshold = median importance)
selector = SelectFromModel(rf, prefit=True, threshold='median')
X_train_sel = selector.transform(X_train)
X_test_sel = selector.transform(X_test)
selected_idx = selector.get_support(indices=True)
selected_features = feat_names[selected_idx].tolist()
print(f"Seçilen feature sayısı: {len(selected_features)}")
print(selected_features)

# 13) Seçilmiş feature'lara göre model yeniden eğit ve performans göster
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report

rf2 = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42, class_weight='balanced')
rf2.fit(X_train_sel, y_train)
y_pred = rf2.predict(X_test_sel)
print("Test accuracy (selected features):", accuracy_score(y_test, y_pred))
print("Classification report:")
print(classification_report(y_test, y_pred))

# 5-fold CV skorları (seçilmiş feature'lar)
cv_scores = cross_val_score(rf2, selector.transform(X), y, cv=5, scoring='accuracy', n_jobs=-1)
print("5-fold CV accuracy (selected features):", cv_scores, "mean:", cv_scores.mean())