# -*- coding: utf-8 -*-
"""pytorch_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xe13xBqzKoGyPfKfbw6neNG9ncCAG68F
"""

import torch
from sklearn.datasets import fetch_california_housing

x = torch.rand(10)

print(x)
print(x.size)


temp = torch.FloatTensor ( [22,23,24,78,99,88] )
print (temp)
print (temp.size() )



housing = fetch_california_housing()
print(housing.data)

# numpy arrayinden torch nesnesine dönüştürme,  pytorch tensor arrayine dönüştü
house = torch.from_numpy(housing.data)
print(house)

# DATASET DOSYASINI GOOGLE DRİVE /tmp klasörüne unzip edilmesi
import shutil
from google.colab import drive

drive.mount("/content/gdrive")

# Filepath kontrol edilmesi
import os

file_path = r"/content/gdrive/MyDrive/kedi.jpg"
if os.path.exists(file_path):
    print("Dosya bulundu.")
else:
    print("Dosya bulunamadı.")

from PIL import Image
import numpy as np
import torch
from matplotlib import pyplot as plt


kedi  = np.array ( Image.open(file_path).resize( (224,224) )   )
keditorch = torch.from_numpy(kedi)

print(np.shape(kedi))
print ( keditorch.size() )

plt.imsave ( "kedi9.jpeg", keditorch[25:150, 125:200,:]  )

import torch

rand1 = torch.rand(2,2)
rand2 = torch.rand(2,2)

print (rand1, rand2)
print (rand1*rand2)

print (rand1.mul (rand2))

import torch

rand1 = torch.rand(10000,10000)
rand2 = torch.rand(10000,10000)

print(".")
rand1.matmul(rand2)

print("...")

# rand1 = rand1.cuda()
# rand2 = rand2.cuda()

#Ders 5 : Lineer Regressyon

import numpy as np
import torch

x = torch.tensor(3.)
w = torch.tensor(4., requires_grad= True)
b = torch.tensor(5., requires_grad= True)

print(x)
print(w)
print(b)

y= w*x +b
print(y)

y.backward()

print ( "dy/dw: " , w.grad)
print(("dy/db: "  , b.grad))

# Inputs ( temp, rainfall, humidity)

inputs = np.array ( [  [73,67, 43] , [91,88,64] , [87,134,58] , [ 102, 43,37] , [69,96,70] ],  dtype='float32')

# Targets ( apples,oranges)

targets = np.array (  [  [56,70], [81,101] , [119,133], [22,37] , [103,119] ]  , dtype='float32' )

inputs = torch.from_numpy(inputs)
targets = torch.from_numpy(targets)

print(inputs)
print(targets)

# Linear Regression Model from scratch

w = torch.randn( 2,3, requires_grad= True)
b = torch.randn(2, requires_grad= True)

print (w)
print (b)

# Model oluşturduk  y=mx+b
def model (x):
  return x @ w.t() + b

preds = model (inputs)
print(preds)

#Loss function kullanarak ne kadar fark çıktı bunu buluyoruz
# mean square error yöntemi kullanarak yapılır

def mse (real, preds):
  diff = real - preds
  return torch.sum(diff * diff) / diff.numel()

loss = mse(targets, preds)
print(loss)
# bunun karekökü alınır ve hesaplanır

#Compute Gradiens

loss.backward()



print(w)
print(w.grad)

print(b)
print(b.grad)

w.grad.zero_()
b.grad.zero_()

print(w.grad)
print(b.grad)

preds = model (inputs)
print(preds)

loss = mse (targets ,preds)
print (loss)

loss.backward()

with torch.no_grad():
  w -= w.grad * 1e-5
  b -=  b.grad * 1e-5
  w.grad.zero_()
  b.grad.zero_()

# with the new weights and biases, the model should have a lower loss

preds = model (inputs)
loss = mse (targets,preds)
print(loss)

# Ders 7 -Lineer Regresyon
#Train for multiple epochs

## eger epoch sayısı yani tekrar saysısı artar ise loss fonksiyonu düşecektir
for i in range (1000):
  preds = model (inputs)
  loss = mse (preds, targets)
  loss.backward()

  with torch.no_grad() :
      w -= w.grad * 1e-5
      b -=b.grad * 1e-5
      w.grad.zero_()
      b.grad.zero_()

preds = model (inputs)
  loss = mse (preds, targets)
  print(loss)

targets

preds

# Ders 8 : Ağ katmanlarını Yazabilmek

import torch
from torch.nn import Linear
import numpy as np

girdi = torch.rand(1)
print(girdi)

Lineer11 = Linear (in_features=1, out_features = 1)

print("Ağırlık w : " , Lineer11.weight)

print ("Bias (y- eksen kestiği yer) : " , Lineer11.bias)

# Ders 8 : Ağ katmanlarını Yazabilmek
print ("Torch ile Lineer : " ,  Lineer11(girdi) )

print("Python ile Hesapladık")
print ( "mx+b ,  m*girdi +b   w*girdi +b ")

print( Lineer11.weight * girdi + Lineer11.bias )

# Ders 8 : Ağ katmanlarını Yazabilmek

Lin1 = Linear (in_features=1, out_features=5 , bias=True)
Lin2 = Linear (in_features=5, out_features=1 )
print("Lin1 : "  )
print(Lin1.weight)


print("Lin2 : "  )
print(Lin2.weight)

print ( Lin2 ( Lin1 (girdi) )  )

# Ders 9: Yapay Sinir Ağımızı Yazalım
import torch
import torch.nn as nn
from sklearn.datasets import load_breast_cancer

device = torch.device("cpu")

#Hyper Parameter

input_size = 30
hidden_size = 500
num_classes = 2
num_epoch = 100

learning_rate = 1e-3

girdi, cikti = load_breast_cancer ( return_X_y=True)


print(girdi)
print(girdi.shape)
print(cikti)
print(cikti.shape)

# PyTorch Dersleri - 14 - Yapay Sinir Ağımızı Bitirelim
#Numpy array den Pytorch array e cevirme
# https://github.com/svishnu88/DLwithPyTorch/blob/master/Chapter3/3.%20Diving%20deep%20into%20Neural%20Networks.ipynb

train_input = torch.from_numpy(girdi).float() # float
train_output = torch.from_numpy(cikti)

class NeuralNet (nn.Module) :

  def __init__ (self,input_size, hidden_size , num_classes):
    super(NeuralNet, self).__init__()
    self.fc1 = nn.Linear (input_size, hidden_size)
    self.lrelu = nn.LeakyReLU (negative_slope=0.02)  # aktivasyon fonksiyonunun tanımlanması
    self.fc2 = nn.Linear ( hidden_size , num_classes )

  def forward ( self,input):
    outfc1 = self.fc1 (input)
    outfc1relu = self.lrelu(outfc1)
    out = self.fc2 ( outfc1relu )
    return out

model = NeuralNet ( input_size, hidden_size, num_classes )  # modelin tanımlanması

# Loss function tanımlanması (crossentropy loss kullandık)
lossf = nn.CrossEntropyLoss()

# Optimizer
optimizer = torch.optim.Adam( model.parameters() , lr =learning_rate )

for epoch in range (num_epoch) :

  outputs = model(train_input )
  loss = lossf ( outputs, train_output)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  print ( 'Epoch [{}/{}] , Loss : {: .4f}'.format(epoch+1, num_epoch, loss.item()   ) )



# Örneğin test etmek isteseydik
# optimizer.zero_grad()
# outputs = model ("resim1.jpg")

# Ders 15 -- PyTorch - 15 - PyTorch ile CNN, CIFAR10


import torch
import torchvision
import torchvision.transforms as tfms

# mean ve std normalize edilidi (mean ,std) --> 3 kanal var rgb

# Veri dönüşümleri
transform = tfms.Compose([
    tfms.ToTensor(),
    tfms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset = torchvision.datasets.CIFAR10 ( root='./data' , train = True , download= True , transform = transform )

# batch size çok alırsanız ram e  güveniyorsunuz demektir, hızlı generalize eder
# batch size düşük alırsanız yavaş çalışır ama accuracy yüksektir, eğitim süresi uzar
# batch size her epoch ta kaç tane veri çekeyim anlamına gelir
trainloader = torch.utils.data.DataLoader ( trainset , shuffle = True , batch_size = 6 , num_workers=2  )  # batch size düşük alırsanız eğitim süresi uzar



testset = torchvision.datasets.CIFAR10 ( root='./data' , train = False , download= True , transform = transform )

testloader = torch.utils.data.DataLoader ( testset , shuffle = True , batch_size = 6 , num_workers=2  )

classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog', 'frog' , 'horse' , 'ship' , 'truck' ) # 10 adet kategori var

import matplotlib.pyplot as plt
import numpy as np

def imshow (img) :
  img = img/2 + 0.5
  npimg = img.numpy()
  plt.imshow ( np.transpose (npimg, (1,2,0)  ))
  plt.show()


dataiter = iter(trainloader)
images, labels = next(dataiter)

imshow (torchvision.utils.make_grid (images) )

# Etiketleri yazdır
print(' '.join(f'{classes[labels[j]]}' for j in range(6)))

# PyTorch - 18 - CNN Ağını Kuralım

import torch.nn as nn
import torch.nn.functional as F


class Net( nn.Module) :

  def __init__(self) :
    super(Net, self).__init__()
    self.conv1 = nn.Conv2d (3,6,3)
    self.conv2 = nn.Conv2d (6,9,3)
    self.conv3 = nn.Conv2d (9,12,3)
    self.pool  = nn.MaxPool2d (  2,2 )
    self.conv4 = nn.Conv2d (12,12,3)
    self.fc1 = nn.Linear (1452 , 10)

# back propagation pytorch kendi hallediyor, biz sadece forward propogation halledeceğiz
  def forward(self,x):
    x = self.conv1(x)
    x = F.relu (x)
    x = F.relu ( self.conv3 ( F.relu (self.conv2(x))))
    x = F.relu ( self.conv4 ( self.pool(x) ))
    x = x.view ( -1, 1452 )
    x = self.fc1(x)
    x = F.softmax(x)
    return x

# PyTorch - 20 - Eğitim İşlemi, Epohchs, Loss, Optimizer
# Loss ve optimizer yazıp sonra da ağımızı eğitelim

net = Net()

import torch.optim as optim

criterion = nn.CrossEntropyLoss()  # bu farklı bir loss fonksiyonda olabilirdi

optimizer = optim.Adam( net.parameters() , lr = 0.001 ) # sgd,adam,rms gibi optimizerlar var


# tekrar kısmı

for epoch in range(10):
  running_loss = 0

  for i, data in enumerate ( trainloader, 0) :
    inputs, labels = data
    optimizer.zero_grad()

    pred = net(inputs)
    loss = criterion (pred , labels )
    loss.backward()
    optimizer.step()

    running_loss = running_loss + loss.item()

    if i % 500 == 0:
      print(" [%d %5d]  loss %0.3f" % (epoch + 1, i + 1, running_loss / 500))